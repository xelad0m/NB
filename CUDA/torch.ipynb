{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кстати тензоры желательно инициализировать..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[140338896403408,  94093785865808,              32,              64],\n",
      "        [ 94105746047532,               0,             112,              64],\n",
      "        [ 94105747574892,               1,               0,             129]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.LongTensor(3, 4)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- vscode-jupyter-toc -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->\n",
    "<a id='toc0_'></a>**Содержание**    \n",
    "- [Autograd](#toc1_)    \n",
    "- [Накопление градиентов](#toc2_)    \n",
    "- [Граф градиентов](#toc3_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- /vscode-jupyter-toc -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Autograd](#toc0_)\n",
    "\n",
    "Tensor object keeps track of how it was created.\n",
    "\n",
    "`z` knows that it was created by the addition of two tensors `z = x + y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  tensor([1., 2., 3.], requires_grad=True)\n",
      "y:  tensor([10., 20., 30.], requires_grad=True)\n",
      "\n",
      "z = x + y\n",
      "z: tensor([11., 22., 33.], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2., 3.], requires_grad = True)\n",
    "print('x: ', x)\n",
    "y = torch.tensor([10., 20., 30.], requires_grad = True)\n",
    "print('y: ', y)\n",
    "\n",
    "z = x + y \n",
    "print('\\nz = x + y')\n",
    "print('z:', z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`s` knows that it was created by the sum of it's numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(66., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "s = z.sum()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У исходных тензоров в атрибуте `grad` появятся частные производные исходя из использованной операции и по форме исходных тензоров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad:  tensor([1., 1., 1.])\n",
      "y.grad:  tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "s.backward()\n",
    "print('x.grad: ', x.grad)\n",
    "print('y.grad: ', y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.), tensor(2.), tensor(4.), tensor(1.), tensor(1.), tensor(1.))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import grad\n",
    "\n",
    "x1 = torch.tensor(2, dtype = torch.float32, requires_grad = True)\n",
    "x2 = torch.tensor(3, dtype = torch.float32, requires_grad = True)\n",
    "x3 = torch.tensor(1, dtype = torch.float32, requires_grad = True)\n",
    "x4 = torch.tensor(4, dtype = torch.float32, requires_grad = True)\n",
    "\n",
    "z1 = x1 * x2 \n",
    "z2 = x3 * x4\n",
    "f = z1 + z2\n",
    "\n",
    "gradients = grad(outputs=f, inputs = [x1, x2, x3, x4, z1, z2])\n",
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **leaf tensor** is a tensor that is created directly and not as a result of any arithmetic operation.\n",
    "\n",
    "In the above case, `x1, x2, x3, x4` are leaf tensors while `z1` and `z2` are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of specifying all the inputs to calculate the gradient using `grad(outputs=f, inputs = [x1, x2, x3, x4, z1, z2])`, we can use `tensor.backward()` to auto calculate all the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of x1 = 3.0\n",
      "Gradient of x2 = 2.0\n",
      "Gradient of x3 = 4.0\n",
      "Gradient of x4 = 1.0\n",
      "Gradient of z1 = None\n",
      "Gradient of z2 = None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/envs/py310/lib64/python3/site-packages/torch/_tensor.py:1104: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten/src/ATen/core/TensorBody.h:475.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.tensor(2, dtype = torch.float32, requires_grad = True)\n",
    "x2 = torch.tensor(3, dtype = torch.float32, requires_grad = True)\n",
    "x3 = torch.tensor(1, dtype = torch.float32, requires_grad = True)\n",
    "x4 = torch.tensor(4, dtype = torch.float32, requires_grad = True)\n",
    "\n",
    "z1 = x1 * x2 \n",
    "z2 = x3 * x4\n",
    "f = z1 + z2\n",
    "f.backward()\n",
    "\n",
    "print(f\"Gradient of x1 = {x1.grad}\")\n",
    "print(f\"Gradient of x2 = {x2.grad}\")\n",
    "print(f\"Gradient of x3 = {x3.grad}\")\n",
    "print(f\"Gradient of x4 = {x4.grad}\")\n",
    "print(f\"Gradient of z1 = {z1.grad}\")\n",
    "print(f\"Gradient of z2 = {z2.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Накопление градиентов](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleLinear(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.Tensor([1]).float(),\n",
    "                                         requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.weight * x  # model: source (W*x) -> target (2*x)\n",
    "\n",
    "def calculate_loss(x: torch.Tensor) -> torch.Tensor:\n",
    "    y = 2 * x           # target\n",
    "    y_hat = model(x)    # source: W*x\n",
    "    loss = (y - y_hat) ** 2\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> zero_grad: (batch 0) : W.grad: None\n",
      "\t-> loss(y=2x, y_hat=Wx) = (2x-Wx)^2/bN/accN = ( 2 * tensor([4.]) - tensor([1.]) * tensor([4.]) ) ** 2 / 2 / 2 = 8.0\n",
      "\t\t-> loss.backward: (batch 0) : W: tensor([1.])\n",
      "\t\t-> loss.backward: (batch 0) : W.grad: tensor([-16.])\n",
      "-> zero_grad: (batch 1) : W.grad: None\n",
      "\t-> loss(y=2x, y_hat=Wx) = (2x-Wx)^2/bN/accN = ( 2 * tensor([2.]) - tensor([1.]) * tensor([2.]) ) ** 2 / 2 / 2 = 2.0\n",
      "\t\t-> loss.backward: (batch 1) : W: tensor([1.])\n",
      "\t\t-> loss.backward: (batch 1) : W.grad: tensor([-4.])\n",
      "\n",
      "optimizer step: (step 2) : W = W - lr*W.grad = W - 0.1 * tensor([-4.]) = tensor([1.4000])\n",
      "\n",
      "-> zero_grad: (batch 2) : W.grad: None\n",
      "\t-> loss(y=2x, y_hat=Wx) = (2x-Wx)^2/bN/accN = ( 2 * tensor([3.]) - tensor([1.4000]) * tensor([3.]) ) ** 2 / 2 / 2 = 1.6200003623962402\n",
      "\t\t-> loss.backward: (batch 2) : W: tensor([1.4000])\n",
      "\t\t-> loss.backward: (batch 2) : W.grad: tensor([-5.4000])\n",
      "-> zero_grad: (batch 3) : W.grad: None\n",
      "\t-> loss(y=2x, y_hat=Wx) = (2x-Wx)^2/bN/accN = ( 2 * tensor([1.]) - tensor([1.4000]) * tensor([1.]) ) ** 2 / 2 / 2 = 0.18000000715255737\n",
      "\t\t-> loss.backward: (batch 3) : W: tensor([1.4000])\n",
      "\t\t-> loss.backward: (batch 3) : W.grad: tensor([-0.6000])\n",
      "\n",
      "optimizer step: (step 4) : W = W - lr*W.grad = W - 0.1 * tensor([-0.6000]) = tensor([1.4600])\n",
      "\n",
      "(final) : W: tensor([1.4600])\n",
      "(final) : grad: tensor([-0.6000])\n"
     ]
    }
   ],
   "source": [
    "model = ExampleLinear()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "data_loader = [torch.tensor([4.0]), torch.tensor([2.0]), torch.tensor([3.0]), torch.tensor([1.0]),]\n",
    "\n",
    "accumulation_steps = 2\n",
    "for i, batch in enumerate(data_loader):\n",
    "    optimizer.zero_grad(set_to_none=True)   # None instead of 0-tensor\n",
    "    print(f\"-> zero_grad: (batch {i}) : W.grad: {model.weight.grad}\")\n",
    "\n",
    "    # The loss needs to be scaled, because the mean should be taken across the whole\n",
    "    # dataset, which requires the loss to be divided by the number of batches.\n",
    "    loss = calculate_loss(batch) / accumulation_steps\n",
    "    print(f\"\\t-> loss(y=2x, y_hat=Wx) = (2x-Wx)^2/bN/accN = ( 2 * {batch} - {model.weight.data} * {batch} ) ** 2 / 2 / {accumulation_steps} = {loss}\")\n",
    "    loss.backward()\n",
    "\n",
    "    print(f\"\\t\\t-> loss.backward: (batch {i}) : W: {model.weight.data}\")\n",
    "    print(f\"\\t\\t-> loss.backward: (batch {i}) : W.grad: {model.weight.grad}\")\n",
    "    if (i + 1) % accumulation_steps == 0:\n",
    "        # Updating the model only after all batches\n",
    "        optimizer.step()\n",
    "        print(f\"\\noptimizer step: (step {i+1}) : W = W - lr*W.grad = W - {optimizer.param_groups[0]['lr']} * {model.weight.grad} = {model.weight.data}\\n\")\n",
    "\n",
    "print(f\"(final) : W: {model.weight.data}\")\n",
    "print(f\"(final) : grad: {model.weight.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> zero_grad: (batch 0) : W.grad: None\n",
      "\t-> loss(y=2x, y_hat=Wx) = (2x-Wx)^2/bN/accN = ( 2 * tensor([4.]) - tensor([1.]) * tensor([4.]) ) ** 2 / 2 / 2 = 8.0\n",
      "\t\t-> loss.backward: (batch 0) : W: tensor([1.])\n",
      "\t\t-> loss.backward: (batch 0) : W.grad: None\n",
      "-> zero_grad: (batch 1) : W.grad: None\n",
      "\t-> loss(y=2x, y_hat=Wx) = (2x-Wx)^2/bN/accN = ( 2 * tensor([2.]) - tensor([1.]) * tensor([2.]) ) ** 2 / 2 / 2 = 2.0\n",
      "\t\t-> loss.backward: (batch 1) : W: tensor([1.])\n",
      "\t\t-> loss.backward: (batch 1) : W.grad: None\n",
      "\n",
      "optimizer step: (step 2) : W = W - lr*W.grad = W - 0.1 * None = tensor([1.4000])\n",
      "\n",
      "-> zero_grad: (batch 2) : W.grad: None\n",
      "\t-> loss(y=2x, y_hat=Wx) = (2x-Wx)^2/bN/accN = ( 2 * tensor([3.]) - tensor([1.4000]) * tensor([3.]) ) ** 2 / 2 / 2 = 1.6200003623962402\n",
      "\t\t-> loss.backward: (batch 2) : W: tensor([1.4000])\n",
      "\t\t-> loss.backward: (batch 2) : W.grad: None\n",
      "-> zero_grad: (batch 3) : W.grad: None\n",
      "\t-> loss(y=2x, y_hat=Wx) = (2x-Wx)^2/bN/accN = ( 2 * tensor([1.]) - tensor([1.4000]) * tensor([1.]) ) ** 2 / 2 / 2 = 0.18000000715255737\n",
      "\t\t-> loss.backward: (batch 3) : W: tensor([1.4000])\n",
      "\t\t-> loss.backward: (batch 3) : W.grad: None\n",
      "\n",
      "optimizer step: (step 4) : W = W - lr*W.grad = W - 0.1 * None = tensor([1.4600])\n",
      "\n",
      "(final) : W: tensor([1.4600])\n",
      "(final) : grad: None\n"
     ]
    }
   ],
   "source": [
    "model = ExampleLinear()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "data_loader = [torch.tensor([4.0]), torch.tensor([2.0]), torch.tensor([3.0]), torch.tensor([1.0]),]\n",
    "MSEloss = torch.nn.MSELoss()\n",
    "\n",
    "accumulation_steps = 2\n",
    "for i, batch in enumerate(data_loader):\n",
    "    print(f\"-> zero_grad: (batch {i}) : W.grad: {model.weight.grad}\")\n",
    "\n",
    "    # The loss needs to be scaled, because the mean should be taken across the whole\n",
    "    # dataset, which requires the loss to be divided by the number of batches.\n",
    "    loss = MSEloss(model(batch), 2*batch) / accumulation_steps\n",
    "    print(f\"\\t-> loss(y=2x, y_hat=Wx) = (2x-Wx)^2/bN/accN = ( 2 * {batch} - {model.weight.data} * {batch} ) ** 2 / 2 / {accumulation_steps} = {loss}\")\n",
    "\n",
    "    print(f\"\\t\\t-> loss.backward: (batch {i}) : W: {model.weight.data}\")\n",
    "    print(f\"\\t\\t-> loss.backward: (batch {i}) : W.grad: {model.weight.grad}\")\n",
    "    if (i + 1) % accumulation_steps == 0:\n",
    "        loss.backward()\n",
    "        # Updating the model only after all batches\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)   # None instead of 0-tensor\n",
    "        print(f\"\\noptimizer step: (step {i+1}) : W = W - lr*W.grad = W - {optimizer.param_groups[0]['lr']} * {model.weight.grad} = {model.weight.data}\\n\")\n",
    "\n",
    "print(f\"(final) : W: {model.weight.data}\")\n",
    "print(f\"(final) : grad: {model.weight.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Общий паттерн: \n",
    "\n",
    "    accumulation_steps = 10\n",
    "    for i, batch in enumerate(batches):\n",
    "        # Scale the loss to the mean of the accumulated batch size\n",
    "        loss = calculate_loss(batch) / accumulation_steps\n",
    "        loss.backward()\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            # Reset gradients, for the next accumulated batches\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "В функции потерь усредняется по фактическому размеру батча, а снаружи добавляется усреднение по \"эффективному\" размеру батча, в рамках готорого обновляются параметры модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Граф градиентов](#toc0_)\n",
    "\n",
    "AUTOGRAD MECHANICS\n",
    "\n",
    "Тензоры собираются в граф, а конкретно дерево - где входные/исходные тензоры соответствуют листьям, а выходные/результирующие - корням.\n",
    "\n",
    "- Концевые узлы графа, терминальные вершины (листья дерева) - тензоры, созданные конструктором. \n",
    "- Узлы ветвления - остальные тензоры, полученные из листьев тензорными операциями, допускающими вычисление производных.\n",
    "\n",
    "Под капотом тут граф объектов-функций.\n",
    "\n",
    "Граф строится по ходу прямого прохода по модели (`forward`). When the forwards pass is completed, we evaluate this graph in the backwards pass to compute the gradients. An important thing to note is that the graph is recreated from scratch at every iteration,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.requires_grad=True\n",
      "a.is_leaf=True\n",
      "b.requires_grad=False\n",
      "b.is_leaf=True\n",
      "c.requires_grad=True\n",
      "c.is_leaf=False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(6., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor(4.0)\n",
    "c = a + b # => tensor(6., grad_fn=<AddBackward0>)\n",
    "\n",
    "print(f\"{a.requires_grad=}\") # => True\n",
    "print(f\"{a.is_leaf=}\") # => True\n",
    "print(f\"{b.requires_grad=}\") # => False\n",
    "print(f\"{b.is_leaf=}\") # => True\n",
    "print(f\"{c.requires_grad=}\") # => True\n",
    "print(f\"{c.is_leaf=}\") # => False\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When defining a custom Python Function, you can use save_for_backward() to save tensors during the forward pass and saved_tensors to retrieve them during the backward pass. See Extending PyTorch for more information.\n",
    "\n",
    "For operations that PyTorch defines (e.g. torch.pow()), tensors are automatically saved as needed. You can explore (for educational or debugging purposes) which tensors are saved by a certain grad_fn by looking for its attributes starting with the prefix _saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, requires_grad=True)\n",
    "y = x.pow(2)\n",
    "print(x.equal(y.grad_fn._saved_self))  # True\n",
    "print(x is y.grad_fn._saved_self)  # True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-determinism\n",
    "\n",
    "При многопоточном выполнении расчета градиентов может возникать гонка за ресурсы и кривые результаты. В этом случае лучше использовать `torch.autograd.grad()` (который что? кажись, он пересчитывает градиенты от листьев к корням) вместо `backward()` (а он, кажись, просто суммирует листья, которые могут поменять в многопоточном варианте).\n",
    "\n",
    "Кастомные градиентные функции (питоновские) потокобезопасны т.к. питоновские (GIL), остальные, написанные на CPP (autograd::Function, в т.ч. torch.autograd.grad()) используют мьютексы для блокировок.\n",
    "\n",
    "Для кастомных CPP функций CPP hooks нужно писать грамотно и потокобезопасно, это проблема уже юзера, на этом полномочия торча все..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hooks for saved tensors**\n",
    "\n",
    "You can control how saved tensors are packed / unpacked by defining a pair of pack_hook / unpack_hook hooks. The pack_hook function should take a tensor as its single argument but can return any python object (e.g. another tensor, a tuple, or even a string containing a filename). The unpack_hook function takes as its single argument the output of pack_hook and should return a tensor to be used in the backward pass. The tensor returned by unpack_hook only needs to have the same content as the tensor passed as input to pack_hook. In particular, any autograd-related metadata can be ignored as they will be overwritten during unpacking.\n",
    "\n",
    "An example of such pair is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/.private/user1'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, uuid, tempfile\n",
    "\n",
    "tmp_dir = tempfile.gettempdir()\n",
    "\n",
    "class SelfDeletingTempFile():\n",
    "    def __init__(self):\n",
    "        self.name = os.path.join(tmp_dir, str(uuid.uuid4()))\n",
    "\n",
    "    def __del__(self):\n",
    "        os.remove(self.name)\n",
    "\n",
    "def pack_hook(tensor):\n",
    "    temp_file = SelfDeletingTempFile()\n",
    "    torch.save(tensor, temp_file.name)\n",
    "    return temp_file\n",
    "\n",
    "def unpack_hook(temp_file):\n",
    "    return torch.load(temp_file.name)\n",
    "\n",
    "tmp_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Registering hooks for a saved tensor**\n",
    "\n",
    "You can register a pair of hooks on a saved tensor by calling the register_hooks() method on a SavedTensor object. Those objects are exposed as attributes of a grad_fn and start with the _raw_saved_ prefix.\n",
    "\n",
    "The pack_hook method is called as soon as the pair is registered. The unpack_hook method is called each time the saved tensor needs to be accessed, either by means of y.grad_fn._saved_self or during the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only save on disk tensors that have size >= 1000\n",
    "SAVE_ON_DISK_THRESHOLD = 1000\n",
    "\n",
    "def pack_hook(x):\n",
    "    if x.numel() < SAVE_ON_DISK_THRESHOLD:\n",
    "        return x\n",
    "    temp_file = SelfDeletingTempFile()\n",
    "    torch.save(x, temp_file.name)\n",
    "    return temp_file\n",
    "\n",
    "def unpack_hook(tensor_or_sctf):\n",
    "    if isinstance(tensor_or_sctf, torch.Tensor):\n",
    "        return tensor_or_sctf\n",
    "    return torch.load(tensor_or_sctf.name)\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n",
    "          # ... compute output\n",
    "          output = x\n",
    "        return output\n",
    "\n",
    "model = Model()\n",
    "net = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5, requires_grad=True)\n",
    "y = x.pow(2)\n",
    "y.grad_fn._raw_saved_self.register_hooks(pack_hook, unpack_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x7fa32d5857b0>>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example what NOT to do (hooks do not go through DataParallel)\n",
    "\n",
    "with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n",
    "    output = net(input)\n",
    "    \n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using those hooks disables all the optimization in place to reduce Tensor object creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0631, 0.4398, 1.6231, 0.7432, 0.3383], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.autograd.graph.saved_tensors_hooks(lambda x: x, lambda x: x):\n",
    "    x = torch.randn(5, requires_grad=True)\n",
    "    y = x * x\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without the hooks, x, y.grad_fn._saved_self and y.grad_fn._saved_other all refer to the same tensor object. \n",
    "\n",
    "With the hooks, PyTorch will pack and unpack x into two new tensor objects that share the same storage with the original x (no copy performed).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75291dc0307ea48294888123147845d2e15abd18d38848ca6ac05a6fe8c88425"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
